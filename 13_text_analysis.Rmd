



# Record Linking and other Data Engineering tasks in R


## demo

See http://rcanada.shinyapps.io/demo and the #GCData2021 Data Engineering workshop  presentation - 
for the backgrounder and the demonstration of various DE tasks and solutions.



# Text Analysis in R

https://gccollab.ca/discussion/view/7404441/text-analysis-in-r

## Open source textbook

https://smltar.com/ 
  https://www.tidytextmining.com/
  https://slcladal.github.io/topicmodels.html, https://slcladal.github.io/textanalysis.html

Also, a list of related resources and codes for text mining (including Web Scraping)  are collected here: https://github.com/gorodnichy/LA-R-text


## Plagiarism detection

Q:  Any ideas/packages/resources (in R) for plagiarism detection? 
  
  
  A: 
  A good  place to start is the "stylo" package (https://github.com/computationalstylistics/stylo - R package for stylometric analyses) which implements a wide variety of recent research in computational stylistics. Plagiarism detection is fraught (insert all of the usual ethical and computational caveats...), but stylo can help you identify passages that are stylistically unusual compared to the rest of the text. Unusualness definitely isn't a proxy for plagiarism, but it's a good place to start.


Q: 
  Is this focused on English language text? Are there lexicons or libraries for comparison within other languages (e.g., French)?
  
  A: Stylo works well with quite a few non-English languages. French, for example, is supported, as are a number of languages with non-Latin alphabets like Arabic and Korean.




## results from International Methodology Symposium

Two presentations  at [International Methodology Symposium](https://www.statcan.gc.ca/en/conferences/symposium2021/program) were about Text Analysis with R, with great ideas from both:
  
  
  11B-4 by Andrew Stelmach from StatCan: used  `library(fastText)` -  a very powerful package from facebook AI team for  efficient learning of word representations and sentence classification. 



11B-2 by Dave Campbell from Carleton U:  used the  approach  that we discussed at L&L on October 9 (based on bag of words cosine distance / correlation) for matching beer products description  - https://gccode.ssc-spc.gc.ca/r4gc/resources/text/),  but addtionally applied SVD (singular value decomposition) to reduce comparison to  the most imporant words, thus reducing significantly the dimension and speed.



You can  find their decks here: https://drive.google.com/drive/folders/1TfuNmG3V8IEKDNNTcMZz7_YCKgqVVBju 


## Useful code snippets

### 


###  Extracting, re-ordering words in a string ----

https://stackoverflow.com/questions/55244680/extract-words-in-between-two-commas-in-r
    
```{r}

word <- 'Fu Tien Mansion, Taikoo Shing, Hong Kong'
# and I want to extract the word in between the two commas and concatenate it with the first word, what regex to use?
# to get this: 'Taikoo Shing Fu Tien Mansion' 

# 1: sub  ----
sub("^([^,]+),\\s*([^,]+),.*", "\\2 \\1", word)
#[1] "Taikoo Shing Fu Tien Mansion"

# 2: -----

x <- base::strsplit(word, ",")[[1]]
paste(x[2], x[1])

# 3 -----
paste(
    trimws( # Remove Leading/Trailing Whitespace
        sapply(strsplit(word,","), `[`, 2)
        ), 
    trimws(sapply(strsplit(word,","), `[`, 1))
    )


### Automatically finding / removing common parts in strings -----


https://stackoverflow.com/questions/48701107/find-length-of-overlap-in-strings
```{r}

findStrOverlap <- function(str1, str2, ignore.case = FALSE) { # , verbose = FALSE

  if(ignore.case) {
    str1 <- tolower(str1);    str2 <- tolower(str2)
  }
  if(nchar(str1) < nchar(str2)) {
    x <- str2;    str2 <- str1;    str1 <- x
  }
    
  x <- strsplit(str2, "")[[1L]]
  n <- length(x)
  s <- sequence(seq_len(n))
  s <- split(s, cumsum(s == 1L))
  s <- rep(list(s), n)

  for(i in seq_along(s)) {
    s[[i]] <- lapply(s[[i]], function(x) {
      x <- x + (i-1L)
      x[x <= n]
    })
    s[[i]] <- unique(s[[i]])
  }

  s <- unlist(s, recursive = FALSE)
  s <- unique(s[order(-lengths(s))])

  i <- 1L
  len_s <- length(s)
  while(i < len_s) {
    lcs <- paste(x[s[[i]]], collapse = "")
    # if(verbose) cat("now checking:", lcs, "\n")
    check <- grepl(lcs, str1, fixed = TRUE)
    if(check) {
        # if(verbose) cat(paste0("Found: '",lcs,"' (length =", nchar(lcs), ") \n")) 
        break
    } else {
      i <- i + 1L 
    }
  }
  return (lcs)
}

library(data.table)
dt <- cansim::get_cansim("13-10-0810-01") %>% setDT(dt) 
dt <- data.table::data.table(
    GEO=c( # From CANSIM Table
        "Newfoundland and Labrador, place of occurrence",
        "Prince Edward Island, place of occurrence",     
        "Nova Scotia, place of occurrence"
))

aStr <- dt$GEO
removeCommonStrPart <- function(aStr) {
    str0 <- findStrOverlap( aStr[1],  aStr[2]); str0
    str_replace(aStr, str0, "")
}

dt[, GEO:=removeCommonStrPart(GEO)][]
#                         GEO
#                      <char>
#1: Newfoundland and Labrador
#2:      Prince Edward Island
#3:               Nova Scotia
```


### Useful packages

`stringdist`
```{r}
stringdist::stringdist(aStr[1],  aStr[2], method="lcs") # 29
stringdist::stringdistmatrix(dt$GEO, dt$GEO, method="lcs") %>% as.data.table 
stringdist::stringdist(aStr[1],  aStr[2], method='qgram',q=2) # 37
```

#### Not as useful ....

`tidystringdist` - converts stringdist results into tibble. not optimized, very slow  
Tip: use data.table instead!


