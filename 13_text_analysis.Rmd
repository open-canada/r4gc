



# Data Engineering, Record Linking and Deduplication 


## TL;DL

- Demo Tool: See http://rcanada.shinyapps.io/demo and the #GCData2021 Data Engineering workshop  presentation \ref{#gc-refs} 
for the backgrounder and the demonstration of various Data Engineering  tasks and solutions. The source is in [GCCode]().

## Intro: What is *Data Engineering*


*Data Engineering* refers to all tasks related to automation of data processing 
within the "insight-from-data" pipeline. 
<!-- It can be thought of as software engineering for data science.  -->

### *Data Engineering* vs. *Software Engineering*

In  analogy to *Software Engineering* (a field of Computer Science  focused on developing
["scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software"[IEEE Vocabulary]](),
developing knowledge, methods robust scalable tools for computer programs),  
*Data Engineering* may  be treated as a field of Data Science that is focused on  developing 
"scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of *data-driven solutions*"]

<!-- 
*Software Engineering*: 

- "the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software" [IEEE Systems and software engineering – Vocabulary]
- "The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software" [IEEE Standard Glossary of Software Engineering Terminology]

- ""software engineering" encompasses not just the act of writing code, but all of the tools and processes an organization uses to build and maintain that code over time. [...] Software engineering can be thought of as “programming integrated over time.”" [Software Engineering at Google]

-->


## Data Engineering vs. ETL and ELT

<!-- https://searchdatamanagement.techtarget.com/definition/Extract-Load-Transform-ELT -->
<!-- With ETL, the raw data is not available in the data warehouse because it is transformed before it is loaded. With ELT, the raw data is loaded into the data warehouse (or data lake) and transformations occur on the stored data. -->
<!-- ELT is most useful for processing the large data sets required for business intelligence (BI) and big data analytics.  -->

DE Data Engineering  with ETL (Extract - Transform - Load  ) but is more than that.

We’ve also observed a general shift away from drag-and-drop ETL (Extract Transform and Load) tools towards a more programmatic approach [ (i.e. Software engineering,Cmputer science approaches)  so that be scalable to  big data] 
To a modern data engineer, traditional ETL tools are largely obsolete because logic cannot be expressed using code.

<!-- 
## Data Engineering vs. Big Data


## Contribution to DE to bias and public trust in AI

--> 

### Taxonomy of Data Engineering tasks

- Single variable 
- Multiple variable, no semantic relationship
- Multiple variable, with semantic relationship




## Useful packages

###  Single variable 


<!-- work in three column bookdown only ```{r results='asis'} -->
<!-- DIV_START("Show / Hide Summary") -->
<!-- ``` -->

<!-- ```{r results='asis'} -->
<!-- DIV_END() -->
<!-- ``` -->

## 0 > R base

### Description


agrep {base}  
adist {utils}


```
agrep(pattern, x, max.distance = 0.1, costs = NULL,
ignore.case = FALSE, value = FALSE, fixed = TRUE,
useBytes = FALSE)
```      

### Examples

```{r}
## Cf. the examples for agrep:
adist("lasy", "1 lazy 2")
## For a "partial approximate match" (as used for agrep):
adist("lasy", "1 lazy 2", partial = TRUE)

x <- c("1 lazy", "1", "1 LAZY")
aregexec("laysy", x, max.distance = 2)
aregexec("(lay)(sy)", x, max.distance = 2)

```



## 1  >`textclean`  {.tabset}




###  Description

textclean: Text Cleaning Tools

Tools to clean and process text. Tools are geared at checking for substrings that are not optimal for analysis and replacing or removing them (normalizing) with more analysis friendly substrings or extracting them into new variables. 

Imports:	data.table, english (≥ 1.0-2), glue (≥ 1.3.0), lexicon (≥ 1.0.0), mgsub (≥ 1.5.0), qdapRegex, stringi, textshape (≥ 1.0.1)

https://www.sciencedirect.com/science/article/abs/pii/S088523080190169X?via%3Dihub
https://github.com/trinker/textclean

### Example
```{r}
library(textclean)

```




## 2 > Package `phonics` {.tabset}

### Description

https://cran.r-project.org/web/packages/phonics/index.html   
https://github.com/k3jph/phonics-in-r ; https://jameshoward.us/phonics-in-r/articles/phonics.html 

Provides a collection of phonetic algorithms including Soundex, Metaphone, NYSIIS, Caverphone, and others. 

Phonetic Spelling Algorithm Implementations for R
James P. Howard, II, Journal of Statistical Software, October 2020, Volume 95, Issue 8. https://www.jstatsoft.org/article/view/v095i08

All functions

caverphone() Caverphone

cologne() Cologne Phonetic Name Coding

lein() Lein Name Coding

metaphone() Generate phonetic versions of strings with Metaphone

mra_encode() mra_compare()Match Rating Approach Encoder

nysiis() New York State Identification and Intelligence System

onca() Oxford Name Compression Algorithm

phonex() Phonex Name Coding

phonics()

Phonetic Spelling Algorithms

rogerroot()

Roger Root Name Coding Procedure

soundex() refinedSoundex()

Soundex

statcan()

Statistics Canada Name Coding

### Example

```{r echo=TRUE}

library("phonics")
x1 <- "Catherine"
x2 <- "Kathryn"
x3 <- "Katrina"
x4 <- "William"

x <- c(x1, x2, x3, x4)

soundex(x1)
refinedSoundex(x1)

(mra1 = mra_encode("Katherine"))
## [1] "KTHRN"
(mra2 = mra_encode("Catherine"))
## [1] "CTHRN"
(mra3 = mra_encode("Katarina"))
## [1] "KTRN"
mra_compare(mra1, mra2)

```


## 3 > Package `stringdist` {.tabset}


### Description

CRAN: https://cran.r-project.org/web/packages/stringdist/stringdist.pdf   (Date/Publication 2020-10-09 10:30:03 UTC)   
URL: https://github.com/markvanderloo/stringdist    
Paper: https://journal.r-project.org/archive/2014-1/loo.pdf,    

Implements an approximate string matching version of R's native 'match' function. Also offers fuzzy text search based on various string distance measures. Can calculate various string distances based on edits (Damerau-Levenshtein, Hamming, Levenshtein, optimal sting alignment), qgrams (qgram, cosine, jaccard distance) or heuristic metrics (Jaro, Jaro-Winkler). An implementation of soundex is provided as well. Distances can be computed between character vectors while taking proper care of encoding or between integer vectors representing generic sequences.

### Example

```{r echo=TRUE}


texts = c("When I grow up, I want to be" , "one of the harvesters of the sea" , "I think before my days are gone" , "I want to be a fisherman"); patterns = c("fish", "gone","to be")

afind(texts, patterns, method="running_cosine", q=3)
grabl(texts,"grew", maxDist=1) 
extract(texts, "harvested", maxDist=3)



amatch(c("hello","g'day"),c("hi","hallo","ola"),maxDist=2)



stringdist(c("g'day"),c("hi","hallo","ola"))

dist <- stringdistmatrix(c("foo","bar","boo","baz")); dist
stats::hclust(dist)

stringsim("Grouchester", "Gloucester")

```



## Multi-variable recording linking

Packages below are listed in the order of the speed and quality





## >> library(fastLink)



### Description

Published:	2020-04-29

Implements a Fellegi-Sunter probabilistic record linkage model that allows for missing data and the inclusion of auxiliary information. This includes functionalities to conduct a merge of two datasets under the Fellegi-Sunter model using the Expectation-Maximization algorithm. In addition, tools for preparing, adjusting, and summarizing data merges are included. The package implements methods described in Enamorado, Fifield, and Imai (2019) ”Using a Probabilistic Model to Assist Merging of Large-scale Administrative Records”, American Political Science Review and is available at <http://imai.fas.harvard.edu/research/linkage.html>.

https://github.com/kosukeimai/fastLink

https://imai.fas.harvard.edu/research/files/linkage.pdf

Imports: parallel, [foreach](https://cran.r-project.org/web/packages/foreach/index.html), doParallel, gtools, [data.table], [stringdist], stringr, stringi, Rcpp


### Dataset

```{r eval=FALSE, include=FALSE}
library(fastLink)
dtA
data(samplematch)

```

### Example

```{r eval=FALSE, include=FALSE}
matches.out <- fastLink(
  dfA = dfA, dfB = dfB, 
  varnames = c("firstname", "middlename", "lastname", "housenum", "streetname", "city", "birthyear"),
  stringdist.match = c("firstname", "middlename", "lastname", "streetname", "city"),
  partial.match = c("firstname", "lastname", "streetname")
)


```


<!-- ## 4. "blink" {.tabset} -->

<!-- ```{r} -->
<!-- library(blink) -->
<!-- ``` -->

<!-- ### Summary -->

<!-- 2020-09-30 -->
<!-- “Entity Resolution with Emprically Motivated Priors”, Bayesian Analysis, (10),4:849-975.  -->
<!-- We will be using the RecordLinkage package in R and the RLdata500 data set. -->
<!-- [link](https://cran.r-project.org/web/packages/blink/vignettes/introEBLink.html) -->



## > Package `RecordLinkage` {.tabset}


### Description

RecordLinkage: Record Linkage Functions for Linking and Deduplicating Data Sets   

Provides functions for linking and deduplicating data sets. Methods based on a stochastic approach are implemented as well as classification algorithms from the machine learning domain. For details, see our paper "The RecordLinkage Package: Detecting Errors in Data" Sariyar M / Borg A (2010) <doi:10.32614/RJ-2010-017>.

Published:	2020-08-25   
Depends:	R (≥ 3.5.0), DBI, RSQLite (≥ 1.0.0), ff    
Imports:	Machine learning - e1071, rpart, ada, ipred, stats, evd, methods,  nnet. Efficiency - data.table (≥ 1.7.8),

Reverse enhances:	SoundexBR



### Datasets: German names 500 and 10,000

The RLdata500 data consists of 500 records with 10 percent duplication.

### Example

Example




## >>>  `library(reclin)`  {.tabset}


### Description

Title: Record Linkage Toolkit

Date: 2018-08-09

Description: Functions to assist in performing probabilistic record linkage and
deduplication: generating pairs, comparing records, em-algorithm for
estimating m- and u-probabilities, forcing one-to-one matching. Can also be
used for pre- and post-processing for machine learning methods for record
linkage.

Depends:	lvec, ldat, R (≥ 3.4.0)
Imports:	stringdist, lpSolve, Rcpp

Comments: Used in DataCamp training

https://cran.r-project.org/web/packages/reclin/reclin.pdf
https://github.com/djvanderlaan/reclin
https://cran.r-project.org/web/packages/reclin/vignettes/introduction_to_reclin.html (with Two small artificial  datasets: first and last names with address, sex, postcode)
https://cran.r-project.org/web/packages/reclin/vignettes/deduplication.html

### Included Datasets: 

Two small (6 and 5 record) artificial names data
```{r}
library(reclin)
data("linkexample1", "linkexample2")
print(linkexample1)

```


### Example 1:

Example


### Example 2: Amsterdam and Rotterdam


```{r results="asis"}
library(reclin)
library(dplyr)
data("town_names")
town_names %>% 
  as.data.table() %>% 
  kable()

```  

####  Deduplication 


```{r}

# https://cran.r-project.org/web/packages/reclin/vignettes/introduction_to_reclin.html

# ...

# https://cran.r-project.org/web/packages/reclin/vignettes/deduplication.html


town_names$clean_name <- gsub("[^[:alnum:]]", "", town_names$name)
town_names$clean_name <- gsub("0", "o", town_names$clean_name)

p <- pair_blocking(town_names, town_names) %>% 
  filter_pairs_for_deduplication() %>%
  compare_pairs("clean_name", default_comparator = jaro_winkler()) %>% 
  score_simsum() %>% 
  select_threshold(0.88)

head(p)
# xtable(p)


res <- deduplicate_equivalence(p)
head(res)


res <- res %>% group_by(duplicate_groups, official_name) %>% mutate(n = n()) %>% 
  group_by(duplicate_groups) %>%
  mutate(group_name = first(official_name, order_by = desc(n)))


precision <- res %>% group_by(group_name) %>% 
  summarise(precision = sum(group_name == official_name)/n())

precision_recall <- res %>% group_by(official_name) %>% 
  summarise(recall = sum(group_name == official_name)/n()) %>%
  left_join(precision, by = c("official_name" = "group_name")) %>% 
  mutate(precision = ifelse(is.na(precision), 0, precision))

precision_recall 


summarise(precision_recall, mean(recall), mean(precision))

``` 

## >>  library(fuzzyjoin)



### Description

fuzzyjoin: Join Tables Together on Inexact Matching  
Join tables together based not on whether columns match exactly, but whether they are similar by some comparison. Implementations include string distance and regular expression matching.

Published:	2020-05-15

Imports:	stringdist

https://cran.r-project.org/web/packages/fuzzyjoin/vignettes/stringdist_join.html

<!-- Comments: Used in DataCamptraining ?-->

### Example 1: Joining with Common Mispelling

```{r echo=T}
# see also dataenh-00-tm.R

library(fuzzyjoin) 
if (F) {
  u <- "https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines"
  h <- read_html(u)
  
  misspellings <- h %>%
    html_nodes("pre") %>%
    html_text() %>%
    readr::read_delim(col_names = c("misspelling", "correct"), delim = ">",
                      skip = 1) %>%
    mutate(misspelling = str_sub(misspelling, 1, -2)) %>%
    unnest(correct = str_split(correct, ", ")) %>%
    filter(Encoding(correct) != "UTF-8")
}
data(misspellings)

setDT(misspellings)


library(qdapDictionaries)
DICTIONARY %>% setDT

words <- as.data.table(DICTIONARY)

set.seed(2016)
sub_misspellings <- misspellings %>%
  sample_n(1000)
joined <- sub_misspellings %>%
  stringdist_inner_join(words, by = c(misspelling = "word"), max_dist = 1)
joined

joined %>%
  count(misspelling, correct)

which_correct <- joined %>%
  group_by(misspelling, correct) %>%
  summarize(guesses = n(), one_correct = any(correct == word))

which_correct

left_joined <- sub_misspellings %>%
  stringdist_left_join(words, by = c(misspelling = "word"), max_dist = 1)

left_joined

left_joined %>%
  filter(is.na(word))

left_joined2 <- sub_misspellings %>%
  stringdist_left_join(words, by = c(misspelling = "word"), max_dist = 2)

left_joined2

left_joined2 %>%
  filter(is.na(word))

```

### Example 2: from datacamp

```{r eval=FALSE, include=FALSE}
# https://rpubs.com/Sergio_Garcia/data_cleaning_in_r
library(fuzzyjoin)
# Create pairs
pair_blocking(zagat, fodors, blocking_var = "city") %>%
  # Compare pairs
  compare_pairs(by = "name", default_comparator = jaro_winkler()) %>%
  # Score pairs
  score_problink()  %>%
  # Select pairs
  select_n_to_m()

```

### Example 3: from stackoverflow

```{r eval=FALSE, include=FALSE}
# https://stackoverflow.com/questions/52402768/r-fuzzy-merge-using-agrep-and-data-table
library("data.table")
dt1 = data.table(Name = c("ASML HOLDING","ABN AMRO GROUP"), A = c(1,2))
dt2 = data.table(Name = c("ASML HOLDING NV", "ABN AMRO GROUP"), B = c("p", "q"))

dt1 = data.frame(Name = c("ASML HOLDING","ABN AMRO GROUP"), A = c(1,2),Date=c(1,2))
dt2 = data.frame(Name = c("ASML HOLDING NV", "ABN AMRO GROUP", "ABN AMRO GROUP"), B = c("p", "q","r"),Date=c(1,2,3))

dt1 %>% fuzzy_inner_join(dt2, by=c("Date","Name"), match_fun=f) %>% filter(Date.x==Date.y)

library(fuzzyjoin)
f <- Vectorize(function(x,y) agrepl(x, y,
                                    ignore.case=TRUE,
                                    max.distance = 0.05, useBytes = TRUE))
dt1 %>% fuzzy_inner_join(dt2, by="Name", match_fun=f)




```




## > Package `blink` {.tabset}


### Description

blink: Record Linkage for Empirically Motivated Priors

An implementation of the model in Steorts (2015) <doi:10.1214/15-BA965SI>, which performs Bayesian entity resolution for categorical and text data, for any distance function defined by the user. In addition, the precision and recall are in the package to allow one to compare to any other comparable method such as logistic regression, Bayesian additive regression trees (BART), or random forests. The experiments are reproducible and illustrated using a simple vignette. LICENSE: GPL-3 + file license.

Depends:	 stringdist

Published:	2020-10-06


https://projecteuclid.org/euclid.ba/1441790411
https://cran.r-project.org/web/packages/blink/vignettes/introEBLink.html


### Datasets: German names 500 and 10,000

### Example

Example



# LSTMs

<!-- [11:29 AM] Ritter, Christian - DSCD/DSCD (Guest) -->

<!-- Gorodnichy, Dmitry  Have you tried using LSTMs for record linkage for entity linkage? We were quiet sucessful in doing it compared to probalistic linkage. -->







# Text Analysis in R

https://gccollab.ca/discussion/view/7404441/text-analysis-in-r

## Open source textbook

https://smltar.com/ 
  https://www.tidytextmining.com/
  https://slcladal.github.io/topicmodels.html, https://slcladal.github.io/textanalysis.html

Also, a list of related resources and codes for text mining (including Web Scraping)  are collected here: https://github.com/gorodnichy/LA-R-text


## Plagiarism detection

Q:  Any ideas/packages/resources (in R) for plagiarism detection? 
  
  
  A: 
  A good  place to start is the "stylo" package (https://github.com/computationalstylistics/stylo - R package for stylometric analyses) which implements a wide variety of recent research in computational stylistics. Plagiarism detection is fraught (insert all of the usual ethical and computational caveats...), but stylo can help you identify passages that are stylistically unusual compared to the rest of the text. Unusualness definitely isn't a proxy for plagiarism, but it's a good place to start.


Q: 
  Is this focused on English language text? Are there lexicons or libraries for comparison within other languages (e.g., French)?
  
  A: Stylo works well with quite a few non-English languages. French, for example, is supported, as are a number of languages with non-Latin alphabets like Arabic and Korean.




## Related work at International Methodology Symposium

Two presentations  at [International Methodology Symposium](https://www.statcan.gc.ca/en/conferences/symposium2021/program) were about Text Analysis with R, with great ideas from both:
  
-  11B-4 by Andrew Stelmach from StatCan: used  `library(fastText)` -  a very powerful package from facebook AI team for  efficient learning of word representations and sentence classification. 

- 11B-2 by Dave Campbell from Carleton U:  used the  approach  that we discussed at L&L on October 9 (based on bag of words cosine distance / correlation) for matching beer products description  - https://gccode.ssc-spc.gc.ca/r4gc/resources/text/),  but addtionally applied SVD (singular value decomposition) to reduce comparison to  the most imporant words, thus reducing significantly the dimension and speed.

You can  find their decks here: https://drive.google.com/drive/folders/1TfuNmG3V8IEKDNNTcMZz7_YCKgqVVBju 


## Useful code snippets

### Basic cleaning : Remove accents (benchmarking)

```{r}

dtCases <- fread("https://github.com/ishaberry/Covid19Canada/raw/master/cases.csv", stringsAsFactors = F )
dtCases %>% dim
system.time(dtCases [, city0 := health_region])
system.time(dtCases [, city1 := base::iconv (health_region, from="UTF-8", to="ASCII//TRANSLIT")])
system.time(dtCases [, city2 := textclean::replace_non_ascii (health_region)])
system.time(dtCases [, city3 := stringi::stri_trans_general (health_region,id = "Latin-ASCII")])
dtCases[city0!=city1, city0:city3] %>% unique


microbenchmark::microbenchmark(  
  dtCases [, city0 := iconv (health_region, to="ASCII//TRANSLIT")],
  dtCases [, city1 := iconv (health_region, to="ASCII//TRANSLIT")],
  dtCases [, city2 := textclean::replace_non_ascii (health_region)],
  dtCases [, city3 := stringi::stri_trans_general (health_region,id = "Latin-ASCII")],
  times=10)

# Unit: milliseconds
# expr       min        lq
# dtCases[, `:=`(city0, iconv(health_region, to = "ASCII//TRANSLIT"))]  166.8094  168.8169
# dtCases[, `:=`(city1, iconv(health_region, to = "ASCII//TRANSLIT"))]  165.9741  168.5937
# dtCases[, `:=`(city2, textclean::replace_non_ascii(health_region))] 8757.1358 8867.7838
# dtCases[, `:=`(city3, stringi::stri_trans_general(health_region,      id = "Latin-ASCII"))] 4204.2102 4230.9790
# mean    median        uq       max neval
# 172.8043  172.4714  174.7670  181.1422    10
# 173.8419  171.0345  172.6113  204.4243    10
# 9088.6954 9049.1962 9273.6495 9545.8301    10
# 4301.7987 4293.4896 4339.4618 4430.2948    10
```

### Text cleaning


library(textclean)
# library(textshape) #with(DATA, split_portion(state, n.words = 10))
# https://github.com/trinker/textclean

if(F) {
  mgsub(textclean::DATA$state, c("i", "it"), c("<<I>>", "[[IT]]"))
  mgsub(DATA$state, "[[:punct:]]", "<<PUNCT>>", fixed = FALSE)
  x <- c(
    "<bold>Random</bold> text with symbols: &nbsp; &lt; &gt; &amp; &quot; &apos;",
    "<p>More text</p> &cent; &pound; &yen; &euro; &copy; &reg;",
    "Welcome to A I: the best W O R L D!",
    "6 Ekstr\xf8m", "J\xf6reskog", "bi\xdfchen Z\xfcrcher",
    "I'm liiiike whyyyyy me?", "Wwwhhatttt!"
  )
  
  replace_html(x)
  replace_kern(x)
  x;  Encoding(x) <- "latin1"; x
  replace_non_ascii(x)
  
  # Tokens
  ## Set Up the Tokens to Replace
  
  lexicon::grady_augmented # 122806 #English words
  
  lexicon::common_names # 5493 #Names
  nms <- gsub("(^.)(.*)", "\\U\\1\\L\\2", lexicon::common_names, perl = TRUE)
  head(nms)
  
  fuzzyjoin::misspellings %>% nrow() #4505
  
  replace_white(x)
  replace_word_elongation(x)
  
  
}

cleanWords <- function(phrase)  {
  # cleanWords <- function(phrase, changecase = c("no", "upper", "lower", "title"))  {
  phrase %>% 
    # gsub("[^[:alnum:]]", "", .) %>%
    gsub("[^[A-Za-z0-9 -]]", "", .) %>%
    gsub(" {2,}", " ", .) 
  # %>%
  # iconv(to="ASCII//TRANSLIT")     
  # replace_non_ascii
  # str_to_lower()
  # ifelse(changecase == "upper", str_to_upper(),
  #        ifelse (changecase == "lower", str_to_lower(),
  #                ifelse(changecase == "title", str_to_title(),
  #                       .)
  #        )
  # )
}

arrayTranslitFromCyrilic <- function (slovo="Дмитрий Городничий") {
  c(
    slovo %>% stri_trans_general("ukrainian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("Russian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("Bulgarian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("Belarusian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("Serbian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("cyrillic-latin")%>% cleanWords
  )
}

if (F) {
  # library(stringi)
  dt <- stri_trans_list() %>% data.table () %>% setnames("coding")#
  dt[coding %ilike% "Latin"][]
  dt[coding %ilike% "cyr"]
  dt[coding %ilike% "ascii"]
  dt1 <- iconvlist() %>% data.table () %>% setnames("coding")#
  dt1[coding %ilike% "ascii"]
  dt1[coding %ilike% "ASCII"]
  dt1[coding %ilike% "TRANSLIT"]
  
  "Montréal" %>% iconv(to="ASCII//TRANSLIT")
  
  c("шерсть","женщина", "челюсть", "Володимир Зеленський", "Дмитрий Городничий") %>% 
    sapply( translitFromCyrilic )
  
}
```


###  Extracting, re-ordering words in a string 

https://stackoverflow.com/questions/55244680/extract-words-in-between-two-commas-in-r
    
```{r}

word <- 'Fu Tien Mansion, Taikoo Shing, Hong Kong'
# and I want to extract the word in between the two commas and concatenate it with the first word, what regex to use?
# to get this: 'Taikoo Shing Fu Tien Mansion' 

# 1: sub  ----
sub("^([^,]+),\\s*([^,]+),.*", "\\2 \\1", word)
#[1] "Taikoo Shing Fu Tien Mansion"

# 2: -----

x <- base::strsplit(word, ",")[[1]]
paste(x[2], x[1])

# 3 -----
paste(
    trimws( # Remove Leading/Trailing Whitespace
        sapply(strsplit(word,","), `[`, 2)
        ), 
    trimws(sapply(strsplit(word,","), `[`, 1))
    )


### Automatically finding / removing common parts in strings -----


https://stackoverflow.com/questions/48701107/find-length-of-overlap-in-strings
```{r}

findStrOverlap <- function(str1, str2, ignore.case = FALSE) { # , verbose = FALSE

  if(ignore.case) {
    str1 <- tolower(str1);    str2 <- tolower(str2)
  }
  if(nchar(str1) < nchar(str2)) {
    x <- str2;    str2 <- str1;    str1 <- x
  }
    
  x <- strsplit(str2, "")[[1L]]
  n <- length(x)
  s <- sequence(seq_len(n))
  s <- split(s, cumsum(s == 1L))
  s <- rep(list(s), n)

  for(i in seq_along(s)) {
    s[[i]] <- lapply(s[[i]], function(x) {
      x <- x + (i-1L)
      x[x <= n]
    })
    s[[i]] <- unique(s[[i]])
  }

  s <- unlist(s, recursive = FALSE)
  s <- unique(s[order(-lengths(s))])

  i <- 1L
  len_s <- length(s)
  while(i < len_s) {
    lcs <- paste(x[s[[i]]], collapse = "")
    # if(verbose) cat("now checking:", lcs, "\n")
    check <- grepl(lcs, str1, fixed = TRUE)
    if(check) {
        # if(verbose) cat(paste0("Found: '",lcs,"' (length =", nchar(lcs), ") \n")) 
        break
    } else {
      i <- i + 1L 
    }
  }
  return (lcs)
}

library(data.table)
dt <- cansim::get_cansim("13-10-0810-01") %>% setDT(dt) 
dt <- data.table::data.table(
    GEO=c( # From CANSIM Table
        "Newfoundland and Labrador, place of occurrence",
        "Prince Edward Island, place of occurrence",     
        "Nova Scotia, place of occurrence"
))

aStr <- dt$GEO
removeCommonStrPart <- function(aStr) {
    str0 <- findStrOverlap( aStr[1],  aStr[2]); str0
    str_replace(aStr, str0, "")
}

dt[, GEO:=removeCommonStrPart(GEO)][]
#                         GEO
#                      <char>
#1: Newfoundland and Labrador
#2:      Prince Edward Island
#3:               Nova Scotia
```


### Useful packages

`stringdist`
```{r}
stringdist::stringdist(aStr[1],  aStr[2], method="lcs") # 29
stringdist::stringdistmatrix(dt$GEO, dt$GEO, method="lcs") %>% as.data.table 
stringdist::stringdist(aStr[1],  aStr[2], method='qgram',q=2) # 37
```

#### Not as useful ....

`tidystringdist` - converts stringdist results into tibble. not optimized, very slow  
Better to use write code yourself to do that using data.table !


```
# > language Detect and convert  ----

# https://stackoverflow.com/questions/8078604/detect-text-language-in-r

# 2.1 ----
library(textcat)
# The textcat Package for n-Gram Based Text Categorization in R. Journal of Statistical Software, http://www.jstatsoft.org/v52/i06/

# 2.2 cld2 - best + archived cran ----

library(cld2) # the fastest of all
# library(cld3) #

if( F) {
  url <- "http://cran.us.r-project.org/src/contrib/Archive/cldr/cldr_1.1.0.tar.gz"
  pkgFile<-"cldr_1.1.0.tar.gz"
  download.file(url = url, destfile = pkgFile)
  install.packages(pkgs=pkgFile, type="source", repos=NULL)
  unlink(pkgFile)
  # or devtools::install_version("cldr",version="1.1.0")
  
  #usage
  library(cldr)
  demo(cldr)
}

# The cldr package in a previous answer is not any more available on CRAN and may be difficult to install. However, Google's (Chromium's) cld libraries are now available in R through other dedicated packages, cld2 and cld3. 

# 2.3 ----
# An approach in R would be to keep a text file of English words. I have several of these including one from http://www.sil.org/linguistics/wordlists/english/. After sourcing the .txt file you can use this file to match against each tweet. Something like:

lapply(tweets, function(x) EnglishWordComparisonList %in% x)


# 2.9 benchmarking - data! ----

data(reuters, package = "kernlab") # a corpus of articles in english
reuters
length(reuters)
# [1] 40
sapply(reuters ,nchar)
# [1] 1311  800  511 2350  343  388 3705  604  254  239  632  607  867  240
# [15]  234  172  538  887 2500 1030  538 2681  338  402  563 2825 2800  947
# [29] 2156 2103 2283  604  632  602  642  892 1187  472 1829  367
text <- unlist(reuters)

microbenchmark::microbenchmark(
  textcat = textcat::textcat(text),
  cld2 = cld2::detect_language(text),
  cld3 = cld3::detect_language(text),
  detect_from_sw = detect_from_sw(text,c("english","french","german")),
  times=10)


# Text analysis ----

# . reading / shaping text ----

# ???
library(textshape)
# https://github.com/trinker/textshape
# textshape is small suite of text reshaping and restructuring functions. Many of these functions are descended from tools in the qdapTools package. This brings reshaping tools under one roof with specific functionality of the package limited to text reshaping.
# This package is meant to be used jointly with the textclean package, which provides cleaning and text normalization functionality. Additionally, the textreadr package is designed to import various common text data sources into R for reshaping and cleaning.

library(textreadr)
# pdf_doc <- system.file("docs/rl10075oralhistoryst002.pdf", package = "textreadr")
# html_doc <- system.file('docs/textreadr_creed.html', package = "textreadr")
# 
#Some other implementations of text readers in R:
# tm
# readtext

'https://github.com/trinker/textreadr/raw/master/inst/docs/pres.deb1.docx' %>%
  download() %>%
  read_docx() %>%
  head(3)
# > Plagiarism detection ----  



# https://cran.r-project.org/web/packages/RNewsflow/vignettes/RNewsflow.html

library(RNewsflow)



#it also installs
library(fastmatch)
library(quanteda)
rnewsflow_dfm 

# https://cran.r-project.org/web/packages/corpustools/vignettes/corpustools.html

library(corpustools)
# corpustools: Managing, Querying and Analyzing Tokenized Text
# Provides text analysis in R, focusing on the use of a tokenized text format. 



# syuzhet: Extracts Sentiment and Sentiment-Derived Plot Arcs from Text
library(syuzhet)


# 5 > Web crawling ----


# fuzzyjoin::misspellings ------
## Not run: 
library(rvest)
library(readr)
library(dplyr)
library(stringr)
library(tidyr)

u <- "https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines"
h <- read_html(u)

misspellings <- h %>%
  html_nodes("pre") %>%
  html_text() %>%
  readr::read_delim(col_names = c("misspelling", "correct"), delim = ">",
                    skip = 1) %>%
  mutate(misspelling = str_sub(misspelling, 1, -2)) %>%
  unnest(correct = str_split(correct, ", ")) %>%
  filter(Encoding(correct) != "UTF-8")

## End(Not run)


## selectr

# https://cran.r-project.org/web/packages/selectr/
#   https://sjp.co.nz/projects/selectr
# 
#   selectr: Translate CSS Selectors to XPath Expressions
# Translates a CSS3 selector into an equivalent XPath expression. This allows us to use CSS selectors when working with the XML package as it can only evaluate XPath expressions. Also provided are convenience functions useful for using CSS selectors on XML nodes. This package is a port of the Python package 'cssselect' (<https://cssselect.readthedocs.io/>).
# 
# Suggests:	testthat, XML, xml2
# Reverse imports:	cliapp, ganalytics, Rcrawler, rvest, wikilake


##  Rcrawler

# Rcrawler: Web Crawler and Scraper
# Performs parallel web crawling and web scraping. It is designed to crawl, parse and store web pages to produce data that can be directly used for analysis application.
# https://cran.r-project.org/web/packages/Rcrawler
# https://github.com/salimk/Rcrawler/
#   RCrawler: An R package for parallel web crawling and scraping,  https://www.sciencedirect.com/science/article/pii/S2352711017300110?via%3Dihub
# 
# Version:	0.1.9-1
# Imports:	httr, xml2, data.table, foreach, doParallel, parallel, selectr, webdriver, callr, jsonlite
# Published:	2018-11-11
# 
# https://cran.r-project.org/web/packages/xml2/index.html
```

### `cleanText(text)`: clean text

#### Data cleaning Tasks:

<!-- https://github.com/trinker/textclean#dates -->

- Stripping non relevant symbols, including spaces
- Contractions and Incomplete words
- abbreviations and slang
- Kerning ("B O M B!") 
- Elongations (looong)
- converting from Non-ASCII, removing accents
- cleaning Numbers: 99,999; 99.999;  99.99%; 99,99%;  2nd 
- normalizing words to a common form (eg Title)
- fixing common typos




<!-- ```{r} -->
<!-- textInput("typedFreeText", "Type anything you want",  -->
<!--           # value=as.character(x),  -->
<!--           width="100%") -->
<!-- ``` -->


<!-- Example:  -->


#### Example 1:

```{r}

x <- c("i like", "<p>i want. </p>. thet them ther .", "I am ! that|", "", NA, 
       "&quot;they&quot; they,were there", ".", "   ", "?", "3;", "I like goud eggs!", 
       "bi\xdfchen Z\xfcrcher", "i 4like...", "\\tgreat",  "She said \"yes\"")
Encoding(x) <- "latin1"
x <- as.factor(x)


renderPrint(x)

```



#### Example 2:

#### 

```{r}
renderPrint(example.text)
```

## ... OUT {.tabset}


### Problem diagnosis: Example 1



```{r}
check_text(x)

```

<!-- # check_text(x) %>% print() %>% data.table() %>% xtable::xtable() -->
<!-- # strip(textclean::DATA$state, apostrophe.remove = TRUE) -->



### Example 2



```{r}

check_text(example.text)

```
  
 ### Test to Date or Timestamp
  
  ```
  
text2date <- function(a) {
  
  a <- a %>% str_replace("XII", "12") %>%
    str_replace("XI", "11")%>%
    str_replace("IX", "9")%>%
    str_replace("X", "10")%>%
    str_replace("VIII", "8")%>%
    str_replace("VII", "7")%>%
    str_replace("VI", "6")%>%
    str_replace("IV", "4")%>%
    str_replace("V", "5")%>%
    str_replace("III", "3")%>%
    str_replace("II", "2")%>%
    str_replace("I", "1")
  
  # x <-  dmy(a); 
  # a <- ifelse(is.na(dmy(a), dym(a), a))
  # a <- ifelse(is.na(dmy(a), dym(a), a))
  # a <- ifelse(is.na(dmy(a), dym(a), a))
  # a <- ifelse(is.na(dmy(a), dym(a), a))
  # a <- ifelse(is.na(dmy(a), dym(a), a))
  # a <- ifelse(is.na(dmy(a), dym(a), a))
  
  x <-  dmy(a);
  if (is.na(x)) x <-  dym(a)
  if (is.na(x)) x <-  ymd(a)
  if (is.na(x)) x <-  ydm(a)
  if (is.na(x)) x <-  mdy(a)
  if (is.na(x)) x <-  myd(a)
  if (is.na(x)) x <-  ymd_hms(a)
  
  if (year(x)>3000) x <- x - years(100)
  return (x)
}
# text2dtDate
text2dtYYMMDD <- function(a, text=F, date=F, timestamp=F) {
  x <- text2date(a)
  dt <- data.table(YY=year(x) %>% as.integer(), MM=month(x)%>% as.integer(), DD=day(x)%>% as.integer() )
  # cols <- 1:ncol(dt)
  # dt[, (cols):=lapply(.SD, as.integer), .SDcols=cols]
  if (text)  dt %<>% cbind(data.table(text=a))
  if (date)  dt %<>% cbind(data.table(date=x))
  if (timestamp)  dt %<>% cbind(data.table(timestamp=now("EST")))
  return(dt)
}


text2timestamp <- function (a) {
  x <-  ymd_hms (a);
  if (is.na(x)) x <- ymd_hm (a)
  if (is.na(x)) x <- ymd_h (a)
  if (is.na(x)) x <- dmy_hms (a)
  if (is.na(x)) x <- dmy_hm (a)
  if (is.na(x)) x <- dmy_h (a)
  if (is.na(x)) x <- mdy_hms (a)
  if (is.na(x)) x <- mdy_hm (a)
  if (is.na(x)) x <- mdy_h (a)
  if (is.na(x)) x <- ydm_hms (a)
  if (is.na(x)) x <- ydm_hm (a)
  if (is.na(x)) x <- ydm_h (a) 
  if (is.na(x)) x <- text2date (a)
  return(x)
}
```
  
 ### Transliteration & cleaning
  
  ```
  cleanPhrase <- function(phrase)  {
  
  phrase %>% 
    gsub("[^[:alnum:]]", "", .) %>%
    gsub("[^[A-Za-z0-9 -]]", "", .) %>%
    gsub(" {2,}", " ", .)   %>%
    iconv(to="ASCII//TRANSLIT")    %>%
    textclean::replace_non_ascii  %>%
    stringr::str_to_lower()
}

cleanWords <- function(phrase)  {
  # cleanWords <- function(phrase, changecase = c("no", "upper", "lower", "title"))  {
  phrase %>% 
    # gsub("[^[:alnum:]]", "", .) %>%
    gsub("[^[A-Za-z0-9 -]]", "", .) %>%
    gsub(" {2,}", " ", .) 
  # %>%
  # iconv(to="ASCII//TRANSLIT")     
  # replace_non_ascii
  # str_to_lower()
  # ifelse(changecase == "upper", str_to_upper(),
  #        ifelse (changecase == "lower", str_to_lower(),
  #                ifelse(changecase == "title", str_to_title(),
  #                       .)
  #        )
  # )
}

arrayTranslitFromCyrilic <- function (slovo="Дмитрий Городничий") {
  c(
    slovo %>% stri_trans_general("ukrainian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("Russian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("Bulgarian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("Belarusian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("Serbian-Latin/bgn")%>% cleanWords,
    slovo %>% stri_trans_general("cyrillic-latin")%>% cleanWords
  )
}
  ```
