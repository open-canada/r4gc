
# Use R efficiently with data.table!
<!-- for very fast and user friendly data processing -->


 ## TL;DR
 
 1. For any code you do (ie, by default) your first lines of code should be:
 ```{r}
 
 
 ```
 

The `data.table` package developed by Matt Dowle is a game changer for many data scientists

Learn about it...  and use it always, 
by default:
```{r}
library(data.table)
dtIris <- data.table(iris) # or
df <- iris; dtIris <- setDT(df)
```


Where to learn: 

- https://github.com/Rdatatable/data.table

- https://github.com/chuvanan/rdatatable-cookbook

 

- http://r-datatable.com (https://rdatatable.gitlab.io/data.table/)
- https://www.datacamp.com/courses/time-series-with-datatable-in-r
- https://www.datacamp.com/courses/data-manipulation-in-r-with-datatable
- https://github.com/Rdatatable/data.table/wiki/Articles
- https://rpubs.com/josemz/SDbf - Making .SD your best friend
 



## data.table vs. dplyr


data.table (Computer language) way  vs. dplyr ("English language") way  

A) The best: No wasted computations .No new memory allocations.
dtLocations %>% .[WLOC == 4313, WLOC:=4312]
B) No new memory allocations, but computations are done with ALL rows.
dtLocations %>% .[, WLOC:=ifelse(WLOC==4313, 4312, WLOC)]
A) The worst: Computations are done with ALL rows. Furthermore, the entire data is copied from  one memory location to another.  (Imagine if your data as in 1 million of cells, of which only 10 needs to be changed !)
dtLocations <- dtLocations %>%   mutate(WLOC=ifelse(WLOC==4313, 4312, WLOC))
NB:  dtLocations %>%  . [] is the same as dtLocations[]. so you can use it in pipes.

 

<!-- Conclusion: Use data.table for speed and efficient coding instead of dplyr (i.e.tibbles)! -->


<!-- Comment  -->

<!-- je côtoie de nombreux scientifiques en environnement qui sont nouveaux dans le monde de la programmation et qui n'ont pas un gros volume de données. Pour eux, dplyr est plus « accessible » que data.table. -->

<!-- Un jour, ils y viendront peut-être, mais en attendant, je vais continuer à enseigner les rudiments de R en proposant dplyr et en leur parlant brièvement du package data.table. -->


## Extensions of data.table

There's considerable effort to marry  data.table package with dplyr package. Here are  notable ones:

- https://github.com/tidyverse/dtplyr (Version: 1.1.0, Published: 2021-02-20, From Hadley himself - I found it quite cumbersome still though...)
- https://github.com/asardaes/table.express (Version: 0.3.1 Published: 2019-09-07 - somewhat easier?)
- https://github.com/markfairbanks/tidytable (Version: 0.6.2 Published: 2021-05-18 - seems to be the best supported of the three?)





# Python and R unite!

Check out our latest Lunch and Learn Meetup  : 
'Lunch and Learn' Meetup: Dual Coding - Python and R unite! : GCcollab

 

It shows how to Execute R from Python and Vice Versa, inspired by this blog: this https://www.kdnuggets.com/2015/10/integrating-python-r-executing-part2.html

 

The used code is available from GCCode:  https://gccode.ssc-spc.gc.ca/r4gc/resources/howto




# From Excel to R

There was a keen interest expressed at last Friday meetup on transitioning from Excel to R. Incidentally, there was an RStudio Community Meet-up focused exactly on this topic:  Meetup: Making the Shift from Excel to R: Perspectives from the back-office

<!-- ## 1 -->

Many Q&A from this meetup are posted here:

https://community.rstudio.com/t/meetup-making-the-shift-from-excel-to-r-perspectives-from-the-back-office/100467

Below are listed some. There's also a link to a video-recording of it! (at link above)


```
What was the most common problem when moving from Excel to R? [Excel -> R Meetup Q&A]
R Markdown vs R Scripts in Enterprise Workflows [Excel -> R Meetup Q&A]
What sort of packages did you use for reconciliation? [Excel -> R Meetup Q&A]
What is the best way to share the results of an analysis that you’ve made in R? [Excel -> R Meetup Q&A]
How do you get buy-in transitioning from Excel to R? [Excel -> R Meetup Q&A]
How do you get buy-in from the IT team around R package security concerns? [Excel -> R Meetup Q&A]
How do you move data from tricky worksheets into R? [Excel -> R Meetup Q&A]
With so many ways of doing the same things, how do you find the best way to do something? [Excel -> R Meetup Q&A]
```


<!-- ## 2 -->


This dictionary of Excel/R equivalent was very useful is finding a starting point for common functions:     
https://paulvanderlaken.com/2018/07/31/transitioning-from-excel-to-r-dictionary-of-common-functions/  

 

For people who are looking for something more comprehensive, this website is very useful:   
https://rstudio-conf-2020.github.io/r-for-excel/



<!-- ## 3 -->

Pour ma part, j'adhère à l'approche proposée par The [Carpentries](https://datacarpentry.org/spreadsheet-ecology-lesson/00-intro/index.html), soit l'utilisation d'Excel pour la saisie de données et une partie du contrôle de la qualité. Par la suite, les données sont exportées en csv pour être importées dans R.



# Reading various kinds of data in R


There are many kinds of data you want to read - and you want to them efficiently (i.e. , automatically and fast):


- badly formatted data, 
- sparse data,

- distributed  "big" data

- just very large and very very large

-  MS excel, MS Words

- from clouds: AWS, MS Azure etc

-  pdf, html

-  zip files

-  google docs, google sheets

-  GCdocs and from other GC platforms (that was one of the questions at our Friday's R meetup), and,

- from web pre-formated

- other IoT and web-crawling


#### fread

Our  favourite methods for reading / writing "regular" .csv files has been 'data.table::fread() and fwrite()' - the fastest and automated in many ways. 

#### vroom

 

Now there's another one - with package 'vroom' - https://cran.r-project.org/web/packages/vroom/vignettes/benchmarks.html

###  rio

```
library(rio)
install_formats()
```

See:  https://www.computerworld.com/article/2497164/business-intelligence-beginner-s-guide-to-r-get-your-data-into-r.html

## readxl and xlsx

For reading Excel files, you can use readxl. 
You would like  to be able to import a set of columns formed by non-contiguous columns of a sheet (something possible to select in the newer versions of Excel using data queries). 

For writing Excel files, you can use xlsx, espcially, if you need to be able to write multiple sheets in a file.


## Discussion


The analyst should, actually, never stick to one solution but rather adapt to the needs of the project. For every format that is good and efficient with big data, you gain either 1) get manipulation overhead that does not make sense when manipulating small datasets, and they can end up slower than even dataframes in that case on small data but hundreds of times faster in big data (e.g. feather), or 2) need to wait forever and lose storage space for nothing (parquet) if the data is not big enough. Yet, if you found the right solution for every size and need, it will make a world of difference.

...

The example below does a comparison of some popular formats when used with Pandas (Python). You will get similar results in R

https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d

 

...

One of the options,  if your are only playing locally and not in the cloud, is using the 'feather' format with 'sql.'

 

If you need to extract data from a database and do more advanced data engineering without loading data in your RAM, you need SQL to prepare the extraction and do basic to advanced manipulation (SQL is Turing-complete, eh).

For more advanced and permanent transformations to the data, you need stored procedures (SQL again).



And if you play in the cloud, this is even more essential. For example, in AWS, you can create user-defined functions in Redshift using Python and Postgres SQL, but not R. All manipulation needs to be done in SQL, and Python can be used for other purposes such as calculations and fuzzy matching.



You can still use R in the rebranded Jupyter notebooks (Sagemaker in AWS, Azure Notebooks in Azure), but R is not as widely compatible in other cloud applications as SQL and Python. - [ PD: But you can absolutely use R in AWS for ETL. In fact you could even set up API endpoints via plumbr, there's a whole AWS tutorial that deals with this issue]

 

References:
 

https://github.com/pingles/redshift-r/
Provides a few functions to make it easier to access Amazon's Redshift service from R.

http://www.rforge.net/RJDBC/index.html
install.packages("RJDBC",dep=TRUE) 
RJDBC is a package implementing DBI in R on the basis of JDBC. This allows the use of any DBMS in R through the JDBC interface. The only requirement is working Java and a JDBC driver for the database engine to be accessed. 
feather  (for larger than gigb):
https://blog.rstudio.com/2016/03/29/feather/

parquet ( for verrrrry large files)
https://campus.datacamp.com/courses/introduction-to-spark-with-sparklyr-in-r/case-study-learning-to-be-a-machine-running-machine-learning-models-on-spark?ex=4


Conclusions: As a side note on size, speed, and performance : it all depends on what you do, the delays, and the cost.

 

For example, if you use the cloud:

-        If your data is going to be queried very often, so you have large volumes of data that would be scanned, move your processing to a runtime-billed tool (e.g. Redshift in AWS) rather than a data-billed tool (e.g. Athena in AWS). Otherwise, your cost may increase exponentially if users can survey data freely from, say, Tableau dashboards without caring for the actual amount of data that is queried. So if the data is queried 24/24h, your cost is stable and won’t increase with volume.

-        If you may scan large volumes once or twice a day, then you would have to compare the costing options.

-        If the costing model is incremental by runtime and you have very large amounts of data that you need to query quickly, then it would be best to use columnar formatted tables such as parquet. There is a cost and delay involved for the conversion, and you need much more storage because of the flattened structure, so storage will be more expensive (especially considering that you clone your original data and use at least twice the space then). However, queries will fly, and the cost of computation will be much smaller thereafter.

-        For occasional queries, a data-billed tool would likely be the best option.

 

If you want to prototype with small datasets, do not lose time with parquet… CSV is the worst format after Excel files (which need to be unpacked and repacked), in any scenario, but the time investment in time to convert data is not worth it at all. Data.table and DT will be your best friends in R.

 

As for using SQL vs packages such as DPLYR, I mentioned a gain in performance, but be careful. If you use raw SQL, then you will see a big gain in performance. However, there are packages out there that translate SQL to R or Python interpretable code, and those will possibly be slower due to the interpretation layer. DPLYR, on the other hand, is quite efficient and well optimized. As usual, it depends on the packages. In R, the sqldf package should be good, if you want to try it out.






# Other tips for  Efficient coding in R 
<!-- (coding style, memory-efficient coding, collaboration-ready codes, source control) -->

<!-- TBA -->

<!-- ## Debugging tricks -->

<!-- TBA -->

## Variable names !

Variable names  need to help you every time you see them! 

Examples:
```{r}


```



### Code formating

Install 'slyler' package to have RStudio addin to automate formatting your code




## Code starter tricks


### Most important / useful libraries

These are libraries / codes that one could use by default when starting  a new code.

```{r}

# 0. General libraries and functions ----

# options(digits = 3); # 7
# options(max.print = 100) # 1000
# options(scipen=999); #remove scientific notation

library(magrittr)
# library(readxl)
# library(readr)
# library(stringr)

## library(tidyverse) # includes:  readr ggplot2 stringr (which we use), dplyr tidyr tibble forcats purrr (may use later) and others - https://www.tidyverse.org/packages/

# library(dplyr) # or
#library(dtplyr)

# library(ggplot2) 
# library(lubridate,  quietly=T) # to work work times and dates
# options(lubridate.week.start =  1)

library(data.table) # needs to be AFTER lubridate
options(datatable.print.class=TRUE)

# library(dygraphs)
# library(plotly)
# library(DT)

`%ni%` = Negate(`%in%`)
```

## Efficient workflows


<!-- Commonly, there are two possible  -->

### Workflow: Data-first approach

<!-- ### Starting with new Data set -->


So, you found a new
<!-- previously not seen  -->
Open Data
dataset. Great - now you can do something with it!  This is how you  start .

```{r}

```




### Workflow: Task/needs/algorithm-first approach

TBA


## Object oriented programming in R



https://techvidvan.com/tutorials/r-object-oriented-programming/


### S3


### R6 







## RStudio tricks



### Coding online

Platforms for coding R online   
<!-- a) https://rextester.com/l/r_online_compiler -->
<!-- b) https://repl.it/languages/rlang (no login required) -->
 https://rstudio.cloud/ (login required) - All in One place for learning and coding



#### Running multiple RStudio versions

You want to be able to run  multiple versions of RStudio in Windows? You can do with the following executable .bat script.

```
# Run-RStudio-1.4.bat

@echo off
title Starting RStudio!
echo Hello Dear,
echo Starting RStudio 1.4 (from C:\Users\gxd006\Downloads\RStudio-1.4.1106\bin\) for you...

C:\Users\abc123\Downloads\RStudio-1.4.1106\bin\rstudio.exe

Set WshShell = CreateObject("WScript.Shell")
WshShell.Run chr(34) & "C:\Users\gxd006\Downloads" & chr(34), 0
Set WshShell = Nothing

```






# Using R with GC data infastructure (gcdocs, AWS, etc)


## gcdocs

Q:

I'm wondering if anyone has had any success accessing data from GCDocs in your respective departments ? I believe that GCDocs is implemented across most (if not all) departments so wondering if there are any existing solutions to read/write data from it.



Also wondering about whether any of you have had any luck accessing Microsoft 365 via R as well? I've had success with Microsoft365R package (https://github.com/Azure/Microsoft365R) from a personal point of view but it doesn't play well with business accounts.


<!-- A: -->
<!--   I tried using the Microsoft365R package to access my departmental (ECCC) email without success. When I tried to access it, a window popped up allowing me to request access authorization so I clicked the "Submit" (or whatever) button. That was weeks ago. Never heard anything more about it. -->

<!-- After trying many ways and weeks, I found this is NOT possible. -  It runs internal code on gcdoc end  that validates that you have right to access it and then, if you do, it also logs your action within the document's "Audit" attribute.  -->
<!-- So we still have to always make a local copy of the data (manually!), and only then we can process it from R. -->


